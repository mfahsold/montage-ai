# Phase 3 Performance Optimization - Research & Implementation Plan

## üî¨ Latest Research Findings (Januar 2026)

### 1. ProcessPoolExecutor f√ºr CPU-Bound Tasks
**Quelle:** Python 3.14 concurrent.futures documentation

**Key Insights:**
- **ProcessPoolExecutor** umgeht den GIL (Global Interpreter Lock)
- **InterpreterPoolExecutor** (Python 3.14+) nutzt Sub-Interpreter f√ºr echte Parallelisierung
- Ideal f√ºr CPU-intensive Aufgaben wie Audio/Video-Analyse
- **Best Practice:** `max_workers=os.process_cpu_count()` f√ºr CPU-Arbeit

**Implementierung:**
```python
with ProcessPoolExecutor(max_workers=cpu_count) as executor:
    futures = [executor.submit(cpu_intensive_task, data) for data in dataset]
    results = [f.result() for f in as_completed(futures)]
```

### 2. Codebase Analyse - CPU-Intensive Bottlenecks

| Modul | Operation | Current | GIL-Bound | Parallelisierbar |
|-------|-----------|---------|-----------|------------------|
| `audio_analysis.py` | Beat detection (librosa) | ThreadPool | ‚úÖ Ja | ‚úÖ ProcessPool |
| `audio_analysis.py` | Energy profiling (NumPy) | Single-threaded | ‚úÖ Ja | ‚úÖ ProcessPool |
| `scene_analysis.py` | Scene detection (PySceneDetect) | ThreadPool | ‚úÖ Ja | ‚úÖ ProcessPool |
| `scene_analysis.py` | Histogram extraction (cv2) | ThreadPool | ‚úÖ Ja | ‚úÖ ProcessPool |
| `clip_selector.py` | Clip scoring | Single-threaded | ‚úÖ Ja | ‚úÖ Vectorized |

### 3. Aktuelle ThreadPool-Nutzung

**Probleme:**
- `ThreadPoolExecutor` wird f√ºr CPU-bound tasks verwendet
- Python GIL verhindert echte Parallelit√§t
- Nur I/O-bound tasks profitieren von Threads

**Gefunden in:**
- `analysis_engine.py:313` - Scene detection (4 workers)
- `analysis_engine.py:361` - AI scene analysis (4 workers)
- `clip_enhancement.py:345` - Enhancement (multi workers)
- `montage_builder.py:287` - General executor (max_workers)

### 4. Binary Serialization (msgpack)

**Vorteile:**
- 40-60% schneller als JSON
- Kleinere Dateigr√∂√üe (30-50% Reduktion)
- Native NumPy-Support

**Use Cases:**
- Cache-Dateien (scenes, beats, energy)
- Inter-process communication
- Metadata-Speicherung

### 5. Content-Addressable Caching

**Strategie:**
```python
cache_key = sha256(file_path + str(mtime) + str(size) + config_hash)
```

**Vorteile:**
- Eliminiert redundante Berechnungen
- Validiert Cache-Freshness
- Config-aware (verschiedene Parameter = verschiedene Keys)

---

## üìä Phase 3 Optimierungen (Priorit√§t)

### üî• High Priority (2-4x Speedup)

#### 1. ProcessPoolExecutor f√ºr Scene Detection
**Datei:** `src/montage_ai/core/analysis_engine.py`

**Problem:** ThreadPool f√ºr CPU-intensive PySceneDetect

**L√∂sung:**
```python
# ALT:
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = [executor.submit(detect_video_scenes, v) for v in videos]

# NEU:
with ProcessPoolExecutor(max_workers=cpu_count) as executor:
    futures = [executor.submit(detect_video_scenes, v) for v in videos]
```

**Expected Impact:** 2-4x schneller bei 4+ Cores

#### 2. ProcessPoolExecutor f√ºr Audio Analysis
**Datei:** `src/montage_ai/audio_analysis.py`

**Problem:** Beat detection und Energy profiling im GIL

**L√∂sung:** Parallel processing f√ºr mehrere Audio-Files

**Expected Impact:** 2-3x schneller bei Multi-Track-Projekten

#### 3. msgpack f√ºr Cache Serialization
**Dateien:** `src/montage_ai/core/analysis_cache.py`

**Problem:** JSON serialization langsam f√ºr gro√üe Datasets

**L√∂sung:**
```python
import msgpack

# Speichern
with open(cache_file, 'wb') as f:
    msgpack.pack(data, f)

# Laden
with open(cache_file, 'rb') as f:
    data = msgpack.unpack(f)
```

**Expected Impact:** 40-60% schneller Caching

### ‚ö° Medium Priority (1.5-2x Speedup)

#### 4. Content-Addressable Cache
**Datei:** `src/montage_ai/core/analysis_cache.py`

**L√∂sung:**
```python
def _compute_cache_key(file_path: str, config: dict) -> str:
    stat = os.stat(file_path)
    config_str = json.dumps(config, sort_keys=True)
    data = f"{file_path}|{stat.st_mtime}|{stat.st_size}|{config_str}"
    return hashlib.sha256(data.encode()).hexdigest()
```

**Expected Impact:** Eliminiert falsche Cache-Hits

#### 5. Batch Processing f√ºr Histogram Extraction
**Datei:** `src/montage_ai/scene_analysis.py`

**Problem:** Frame-by-frame extraction ineffizient

**L√∂sung:** OpenCV batch frame reader

**Expected Impact:** 1.5-2x schneller

### üí° Low Priority (Erg√§nzungen)

#### 6. Explicit GC Collection
**√úberall wo gro√üe Objekte freigegeben werden**

```python
import gc
# Nach gro√üen Operationen
del large_object
gc.collect()
```

#### 7. Memory-Mapped Files f√ºr gro√üe Cache-Files
**F√ºr Histogram-Cache**

```python
import mmap
# F√ºr sehr gro√üe Dateien
with open(file, 'r+b') as f:
    mmapped_file = mmap.mmap(f.fileno(), 0)
```

---

## üéØ Implementation Strategy

### Phase 3.1: ProcessPoolExecutor (CPU Parallelization)
1. ‚úÖ Scene detection ‚Üí ProcessPool
2. ‚úÖ Audio beat detection ‚Üí ProcessPool
3. ‚úÖ Energy profiling ‚Üí ProcessPool

### Phase 3.2: Serialization Optimization
4. ‚úÖ msgpack integration
5. ‚úÖ Content-addressable caching

### Phase 3.3: Memory & Batch Optimization
6. ‚úÖ Batch histogram extraction
7. ‚úÖ Explicit GC management

---

## üìà Expected Combined Impact

| Phase | Optimizations | Expected Speedup | Cumulative |
|-------|---------------|------------------|------------|
| Phase 1 | FFmpeg astats, LRU cache | 1.5-2x | 1.5-2x |
| Phase 2 | Keyframes, RAM disk, vectorization, K-D tree | 2-3x | 3-6x |
| **Phase 3** | **ProcessPool, msgpack, content cache** | **2-4x** | **6-24x** |

---

## üîß Technical Details

### ProcessPool vs ThreadPool

**ThreadPoolExecutor:**
- ‚úÖ Low overhead
- ‚úÖ Shared memory
- ‚ùå GIL-bound (kein echtes Parallelism f√ºr CPU-Tasks)

**ProcessPoolExecutor:**
- ‚úÖ Echtes Parallelism (umgeht GIL)
- ‚úÖ Nutzt alle CPU-Kerne
- ‚ùå Pickle-Overhead f√ºr Daten√ºbertragung
- ‚ùå Mehr Memory (separate Prozesse)

**Faustregel:**
- CPU-bound ‚Üí ProcessPoolExecutor
- I/O-bound ‚Üí ThreadPoolExecutor
- Mixed ‚Üí ProcessPool f√ºr CPU-Teil, ThreadPool f√ºr I/O

### Pickle-Kompatibilit√§t

**F√ºr ProcessPool m√ºssen Funktionen picklable sein:**
- ‚úÖ Top-level Funktionen
- ‚úÖ Lambdas (in Python 3.8+)
- ‚úÖ Klassen-Methoden (mit `__reduce__`)
- ‚ùå Verschachtelte Funktionen
- ‚ùå Lokale Closures mit komplexem State

**L√∂sung:** Extract zu Modul-Level Funktionen

---

## üß™ Benchmark Targets

### Before Phase 3
```
Audio Analysis:    369ms (Phase 1 optimized)
Scene Detection:   ~2000ms (keyframes)
Clip Selection:    ~50ms (vectorized)
Cache Operations:  ~100ms (JSON)
```

### After Phase 3 (Target)
```
Audio Analysis:    120-180ms (2-3x via ProcessPool)
Scene Detection:   500-1000ms (2-4x via ProcessPool)
Clip Selection:    ~50ms (bereits optimiert)
Cache Operations:  40-60ms (2.5x via msgpack)
```

### Total Expected Improvement
**Baseline (Phase 0):** ~3000ms  
**Phase 1+2:** ~1000ms (3x)  
**Phase 3 Target:** ~300-500ms (6-10x vs Baseline)

---

## ‚úÖ Implementation Checklist

- [ ] ProcessPool f√ºr scene detection
- [ ] ProcessPool f√ºr audio beat detection
- [ ] ProcessPool f√ºr energy profiling
- [ ] msgpack integration in analysis_cache
- [ ] Content-addressable cache keys
- [ ] Batch histogram extraction
- [ ] Explicit GC in critical sections
- [ ] Benchmark Phase 3 improvements
- [ ] Update documentation

---

**Research Date:** 7. Januar 2026  
**Python Version:** 3.12.3 (3.14 features noted for future)  
**Target:** < 500ms critical path latency
